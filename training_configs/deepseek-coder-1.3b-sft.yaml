# Model arguments
model_name_or_path: deepseek-ai/deepseek-coder-1.3b-base
torch_dtype: bfloat16
attn_implementation: flash_attention_2

chat_template: "{% if not add_generation_prompt is defined %}\n{% set add_generation_prompt = false %}\n{% endif %}\n{%- set ns = namespace(found=false) -%}\n{%- for message in messages -%}\n    {%- if message['role'] == 'system' -%}\n        {%- set ns.found = true -%}\n    {%- endif -%}\n{%- endfor -%}\n{{bos_token}}{%- if not ns.found -%}\n{{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n'}}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message['role'] == 'system' %}\n{{ message['content'] }}\n    {%- else %}\n        {%- if message['role'] == 'user' %}\n{{'### Instruction:\\n' + message['content'] + '\\n'}}\n        {%- else %}\n{{'### Response:\\n' + message['content'] + '\\n<|EOT|>\\n'}}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}\n{{'### Response:'}}\n{% endif %}"
dataset_mixer:
  ${YOUR_DATA_PATH}: 1.0
dataset_splits:
- train
- test
preprocessing_num_workers: 1

# SFTTrainer arugments
bf16: true
do_eval: true
eval_strategy: epoch
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: False
learning_rate: 5.0e-06
log_level: info
logging_steps: 100 
logging_strategy: steps
lr_scheduler_type: linear
max_seq_length: 2048
max_steps: -1
num_train_epochs: 5
output_dir: outputs/deepseek-coder-1.3b-sft
overwrite_output_dir: true
per_device_train_batch_size: 8
per_device_eval_batch_size: 4
push_to_hub: false
remove_unused_columns: true
save_strategy: "steps"
save_steps: 100000000
save_total_limit: 1
seed: 42
warmup_ratio: 0.1